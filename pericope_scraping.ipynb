{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "NqWnjkRzE-W3"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "SVQl_tVlNkVt"
      },
      "outputs": [],
      "source": [
        "#get USFM identifiers\n",
        "URL = 'http://ubsicap.github.io/usfm/identification/books.html'\n",
        "page = requests.get(URL)\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "stuff = [node.getText().split('\\n')[1] for node in soup.findAll(class_=['row-odd', 'row-even'])]\n",
        "book_codes = stuff[1:67]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "Hk_KRWs7HDOQ"
      },
      "outputs": [],
      "source": [
        "#get list of links to book pages\n",
        "URL = 'https://www.biblestudystart.com/outlines/'\n",
        "page = requests.get(URL)\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "links = []\n",
        "\n",
        "for i, node in enumerate(soup.findAll('li')):\n",
        "\n",
        "  link = node.find('a')\n",
        "\n",
        "  if link is not None:\n",
        "    fullLink = URL + link.get('href')\n",
        "    links.append(fullLink)\n",
        "\n",
        "#last two links are not relevant in this case\n",
        "del links[-1]\n",
        "del links[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "lWm261bcIWmN"
      },
      "outputs": [],
      "source": [
        "#create lists for all the fields we want in the end \n",
        "all_book_codes = []\n",
        "all_chapters = []\n",
        "all_start_verses = []\n",
        "all_end_verses = []\n",
        "all_summaries = []\n",
        "\n",
        "#iterate through the link for each book\n",
        "for i, link in enumerate(links): \n",
        "\n",
        "  new_page = requests.get(link)\n",
        "  new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
        "\n",
        "  #this string is formatted chapter:start_verse-end_verse\n",
        "  chap_verse = [verse.getText().split() for verse in new_soup.findAll(class_ = \"verses\")]\n",
        "\n",
        "  #need to fill in chapter info for Obadiah, where there is only one chapter and the website only lists verses\n",
        "  for c_v in chap_verse:\n",
        "    if len(c_v[1].split(':')) == 1:\n",
        "      c_v[1] = \"1:\"+c_b[1]\n",
        "\n",
        "  #get chapter info\n",
        "  chapters = [c_v[1].split(':')[0] for c_v in chap_verse]\n",
        "  verses = [c_v[1].split(':')[1] for c_v in chap_verse]\n",
        "\n",
        "  #get start and end verses\n",
        "  start_end = [v.split('-') for v in verses]\n",
        "\n",
        "  #if there is only one verse listed, list it as both start and end\n",
        "  for s_e in start_end:\n",
        "    if len(s_e) == 1:\n",
        "      s_e.append(s_e[0])\n",
        "\n",
        "  #lists all info per book\n",
        "  book_codes_i = [book_codes[i] for j in range(len(start_end))]\n",
        "  start_verses = [s_e[0].split('.')[0] for s_e in start_end]\n",
        "  end_verses = [s_e[1].split('.')[0] for s_e in start_end]\n",
        "  summaries = [summary.getText() for summary in new_soup.findAll(class_ = \"summary\")]\n",
        "\n",
        "  #append to the all lists\n",
        "  for j in range(len(book_codes_i)):\n",
        "    all_book_codes.append(book_codes_i[j])\n",
        "    all_chapters.append(chapters[j])\n",
        "    all_start_verses.append(start_verses[j])\n",
        "    all_end_verses.append(end_verses[j])\n",
        "    all_summaries.append(summaries[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "-rxxr-dhJpJL"
      },
      "outputs": [],
      "source": [
        "#make df\n",
        "data = {\n",
        "    'Book':all_book_codes,\n",
        "    'Chapter':all_chapters,\n",
        "    'Start Verse':all_start_verses,\n",
        "    'End Verse':all_end_verses,\n",
        "    'Summary':all_summaries,\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "kHGChRvwLt5p"
      },
      "outputs": [],
      "source": [
        "#to csv\n",
        "df.to_csv('pericopes.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "pericope_scraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
